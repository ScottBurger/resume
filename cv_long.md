# Scott V Burger Curriculum Vitae
(an exhaustive list of deliverables)


## Employment History

### Senior Data Scientist, Engineering Analytics - Qualtrics (Oct 2019 - Oct 2024)
Developed the centralized reporting dashboard and infrastructure for C-Suite visibility into company-level metrics such as: Ease of Use, Pace of Interactions, Time to Value, Number of Interactions, Monthly Active Users, Daily Active Users, Monthly Active Brands, Jira Pulse SLAs, Data Center Availability, Service Level Objectives, Security Patching, and 20 other main metrics used for criticial success measurement across Product, Experience, and Engineering. Accomplished with Python, Prefect, and Tableau.

Leading critical operational metrics of 50+ different KPIs visible to Engineering leadership and C-Suite. 40% of all engineers in the company have interacted with Eng Analtyics data products and 35% of the entire company overall. For our most recent quarterly review, a majority of users were saved a great deal of time and had a great deal of decision making process influenced by our Tableau dashboards, Redshift tables, and Airflow DAGs.

Managed mission-critical operation of operational review metrics used in realtion to Jira data. Major investigations and realtime reporting on customer pulses, deep dives on product quality feedback, and analyses of backlog management across all Engineering teams.

Investigated OKR adoption practices and made a company-wide change for OKRs to be more tightly linked via manager-direct objective-result pair linking.

Identified 140 brands to be targeted by the Qualtrics Preview Program with a net revenue lift of $14M+.

Led an investigation into pre- and post-pandemic Engineering efficiency metrics and found that Qualtrics transitioned with no statistically significant impact on developer productivity. Led initiatives on data governance to establish the adoption of Alation and data documenation best practice updates.

Led an initiative to bring analytics team across the company together for informal tech talks, averaging two per month.

---

Developed and maintained major canonical data sources related to Salesforce accounts and configure-price-quote information. This enabled Sales Operations and GTM teams to better understand where to target for business planning questions, weaknesses in our product taxonomy, and strategies for better data alignment between teams.

Instrumented and owned best practices for aligning business planning data hosted in Coda to Redshift data tables in the data warehouse. Accomplished via Coda API calls and automation via Prefect. This enabled a snapshotted set of data to see evolutions in product taxonomy at specific points in time, enabled use of joining the product data to other sources like Salesforce, and enabled new simplified product taxonomy groupings for established metrics and reports.

Worked closely with TPMs from GTM and Sales Operations to drive requirements for data quality related to Salesforce licensing management systems. This established a baseline for data quality, giving us a number to understand current account coverage per license, and evaluate strategies for attributing usage and consumption metrics to a given account's active license bundles.

Delivered multiple quarters worth of Tableau training seminars to internal teams to help upskill employees who had been mostly leveraging Redash for BI reporting purposes.

Mentored 16 people through the Q-Mentor program, ranging from junior and intern-level data scientists, to senior and staff level technical program managers. I recieved highly favorable reviews from mentees and would often extend the mentorship program beyond its originally scoped timeframe in order to continue the program, per mentee requests.

Upgraded 137 internal company reports and dashboards to a new framework leveraging enhanced data governance principles via internal wiki management and internal developer tooling.

Managed and operated a brand admin contact data workflow that was critical for corporate communication between engineering teams and externally-facing communications departments. This system allowed non-techncial users to reach out directly to brand admininstrators to keep them updated on important features and changes. This had visibility up to the Chief People Officer.

Led several initiatives for upgrading and evaluating alternatives for internal business intelligence reporting software including Tableau, AWS Quicksight, Google Looker, and Apache Superset. Ultimately landed on continuing to use Tableau as our reporting platform and drove requirments to have additional reporting moved off of Redash and into the Tableau ecosystem.

Reported a deep dive analysis of employee engagement data showing major factors and trends regarding return-to-office logistics. Most employees onboarded since full remote work missed having more social team gatherings, and a major inhibitor to return-to-office being commute timing.

Developed and maintained a scorecard for engineering managers that would track OKR attainment, organization size, DevOps related metrics for things like commit frequency, customer pulse data for any given manager's organization, and other kinds of employee engagement data.

Developed and maintained numerous dashboards and data sources for the Launch Readiness program that enabled the team to understand how effective and efficient the program is currently, and how it has evolved over time. This initiative drove major changes the the launch process and simplified processes considerably for many engineering teams.

Developed and owned an engineering-wide metric for the number of features adopted per team per quarter. Established data baselines and set target goals for engineering teams to meet based on historical data trends.

Automated a major headcount data workflow for engineering managers, saving a great deal of time for L6s and above involved in the hiring process.

Developed and owned an engineering-wide processes scorecard that documented 76 KPIs across organization metrics, outcome metrics, and process metrics.

Developed a process to understand data product impact and found that my dashboards and reports had saved staff in engineering upwards of 500 hours of time per quarter and about 260 hours of non-engineer time.

Upgraded and optimized many engineering data sources from an OLTP instance of AWS Aurora to OLAP Redshift. Many of which saving upwards of 40 hours of maintenance time per quarter, simplifying ETLs to be easier to debug and understand, and allowing more expansive feature sets to be built on top of them in the future. 

Led an analysis of how renewal rates tie to a brandâ€™s Preview Program membership to indicate big advances in various client tiers, with a net positive impact. I showed upwards of 14% renewal increase in high tier accounts and upwards of 113% in mid-level tier accounts.

Developed and owned a dynamic ownership mapping system of internal operations data to identify "who owns what" based on organizational hierarchies. Engineering managers were able to see in databases which programs were owned by who at what time and how organizational charts evolved over time.

Developed and owned a more accessible frontend API interface to the Qualtrics YellowPages project. This enabled engineering program managers to export data from internal APIs in a more useful and straightforward way as opposed to writing individual API queries to retrieve data on program ownership and details.

Performed an analysis on several Qualtrics products and tied customer satisfaction to them to understand which ones were driving positive PSAT and which ones were driving negative PSAT. This was used by program managers to further understand customer frustrations and optimize user journey flows for better product experiences.

Owned and maintained documentation on analytical best practices at Qualtrics, ranging from query optimization guides on AWS systems, managing python and R connections to data platforms, best practices on Bi and reporting structure, data analytics interview question design, and resources for growing data and analytics skillsets.

Performed behavior and techcnical interview screens for 20 applicates for various roles at Qualtrics, ranging from junior to senior level data positions.

Owned and operated over 100 enginering data sources and dashboards related to top-level KPIs with domains ranging from Jira customer pulse data, Launch Readiness Review data, engineering QA and testing data, internal operations data related to things like engineering efficiency and organizational data, DevOps metrics, and engineering infrastructure metrics like data center reliability and availability.

Productionalized a predictive model for Jira pulse volume prediction. This enabled engineering QE teams and frontline ticket support to better understand when to expect high volumes of support requests from customers and at what severity level.

Productionalized a Jira pulse volume anomaly detection model for engineering teams. This helped to set real-time alerts for when customer ticket creation volumes spiked for specific teams, but also showed when teams were experiencing lower than normal ticket volumes, so both sides of the RCA success story could be told.

Developed and operated a plan across numerous teams in engineering to onboard and adopt AWS Redshift as the internal standard for company-wide data warehousing. Drove requirements for better analytics framework best practices, including formalizing metrics and report specific schemas in the new data warehouse. This helped to save massive amounts of time and effort from originally developing analytics queries on the OLTP framework of AWS Aurora, which at the time did not support even CTE functionality.

Worked with HR and recruiting to develop internal headcount reporting dashboards used for budget and planning purposes across engineering and other orgs at the company. Visibily up to C-Suite to help drive decisions on organizational management.

Productionalized a predicitive hiring model for engineering to assess growth of the organization at various seniority levels and track potential growth for various engineering teams.

Developed and maintained data sources and dashboards to track engineering-wide OKR attainment at all management levels. Visibility up to CTO helped enable standardization of org-wide best practices and maintain alignment with working procedures to show a clear chain of how individual objectives ladder up to org-wide initiatives.

Developed and owned standardized best practice documentation for usage of Presto and Data Lake. This initiative helped to evangelize use cases for each of these systems to other data users, reduce overhead for the core data engineering team, and ultimately drive data end users to the best ecosystem that suits their use cases.

Owned and operated a data governance v-team across numerous organizations including engineering, revenue operations, finance, security, data engineering, and sales operations.

Productionalized and owned a data source for tracking internal company wiki pageview statistics. This helped teams across the company understand how useful their documentation was overall, who the major consumers of their documentation were, and how they were trending over time.

Drive company best practices for internal reporting standards. Worked with internal development operations teams to move over 100 reports from a system managed by a siloed centralized team into the self-serve company-wide internal wiki. This helped drastically reduce update and KTLO time for the internal development teams, and helped enable many new useful features including data governance applications. A centralized structure helped realign company reporting in a one-stop-shop area so senior leadership could easily navigate various company KPIs.

Owned and maintained over two dozen airflow automation pipelines critical for engineering data success related to Jira data, GitLab DevOps metrics, and other sources of data.

Performed a research investigation into potential hiring bias in the engineering organization for historically underrepresented groups (HUGS) and found no statistically significant areas of concern.

Owned and maintained data sources and reporting infrastructure related to engineering data center reliability data. This helped engineers and leadership understand where issues were happening, which teams were responsible, SLAs for action, and associated metrics.

Productionalized and maintined data sources and reporting for engineering QA sytems including Testrail. This helped engineers understand where unit, integration, and end-to-end tests were happening with a high degree of granularity down to specific teams, but also being able to be aggregated up to specific engineering managers as well. 

Helped foster and develop the data engineering-led initiative "Data Champions" which nominated representatitves from across the company's organizations to serve as data experts for specific areas of focus.


### Senior Marking Analyst - Tableau Software (Feb 2018 - Oct 2019)

Productionalized and maintained a several multi-touch attribution models on Google BigQuery, with additional versions developed in R, with data pushed to tables for downstream systems to utilize.

Developed and maintained data sources related to customer journey path analysis in BigQuery.

Led and maintained office hours support for other parts of the marketing organization to come and ask technical questions, get help, and learn to self-serve for various data-related issues.

Owned and operated data sources related to major marketing product launches.

Developed models to understand and integrate various lead scoring models like SixSense into our system and understand how interally developed tools compared.

Handled data science interviewing, onboarding, and intern transitions to full time employment.

Owned and maintained data sources critical to the Marketing organizationâ€™s daily operation such as opportunity data, lead contact data, and quota attainment.

Developed and productionalized models to predict customer lifetime value based on account level data usage and consumption.




### Data Scientist - Microsoft (May 2014 - Jan 2018)

Worked across many teams including Visual Studio Team Services, Azure, Engineering Customer Interactions, Data & Decision Sciences.
Developed data engineering pipelines in Kusto, Dax, and SSIS to provide high-visibility insights in PowerBI to stakeholders in numerous product organizations including Legal, Xbox, and Support.

Led trainings on data science and analytics best practices with R, SPSS, SAS, and various data visualization tools.

#### Visual Studio Team Services 
Worked on advanced analytics and machine learning in the Visual Studio Team Services group by writing Kusto queries to answer business questions with statistical data modelling. The reporting conclusions have had visibility to senior leadership. Examples include analyses of user behavior, seasonality of KPIs, anomaly detection, mapping user journeys, visualization, licensing investigations, and documentation of key business insights.

#### Azure Identity FastTrack Analytics Team
With this group with Azure Active Directory, I was tasked with rebuilding the entire end-to-end data pipeline to provide a stream of insight to feed our analytics dashboards. I rebuilt the system to have all the data processing done in the cloud instead of in a local system. I leveraged R code for machine learning insights into the data. From this work, we were able to evolve the group from dashboarding static reports to generating trends and insights in the data to use for predictive purposes.

#### Retail and Channel Marketing Team 
Microsoft's Retail and Channel Marketing team focuses on sales unit modelling for various product groups across a very stratified set of data, including several different form factors, distributors, and supply-chain business logic scenarios. With the sales team, I've led a small team of data scientists to guide modelling approaches as well as designing analytical models myself. We have successfully completed many business objectives ranging from data warehousing strategy to complex machine learning model forcecasts.

The team had originally been relying on Excel as a crutch for all their data needs. One of our most impactful fixes was to use a more intelligent SQL system to warehouse the data. Another key impact we drove was to utilize machine learning association rules to better understand our data and how to reccomend it to customers.

The highest impact deliverable for this project was developing a market sizing model to determine where gaps are within internal data and how to approach an extrapolation to the full market picture with minimal reliance on third party data. The model is primarially a data reshaping exercise that is built in an R enviornment. We use the final flat sizing model table as a base layer for visualization to key business stakeholders and to help drive decision making. 

 I have done extensive work in applying machine learning modelling to sales projections from our data warehousing outputs. By tying together different datasets from vastly different business ends of the spectrum and reshaping appropriately, we first build a unified data structure across multiple verticals including time, form factor (ie, desktop or notebook), and sales channel, we can then apply GDP-based economic factors to the data and train our machine learning models accordingly. We have done this exercise with great accuracy for predicting unit sales at a yearly and quarterly level. 

Other projects in this space include theoretical economic market cap estimation, ARIMA modelling and sesonality decomposition for use in sales forecasting, cross-channel retail consumption models based on extracted linear models for each vertical slice of the data, and designing sales growth ranking algorithms to bucketize distributiors into high, medium, and low performance categories. 

#### KAT-BI Team
The KAT-BI team is responsible for BI and analytics across multiple high-visibility sites across Microsoft, including MPO, MPN, MSDN, SMC, SSB, and TechNet. Across these sites, I helped manage a team that was responsible for scorecard design and maintenance. We designed many different survey techniques to give visibility into NSAT metrics across the sites, as well. These designs ranged from A/B tests on currently deployed methodologies, to implementing new design methods for survey feedback and scoring. The scorecard infrastructure was designed in Cosmos, while being fed into a dashboard that was updated in PowerBI. 

My end deliverable for this project was to develop a simpler feedback response mechanism. I worked with design teams and developers to implement a new feedback system that was then integrated into the scorecard's NSAT tracking.


#### Engineering Customer Interactions and IT
With the highly agile and fast-paced Engineering, Customer interactions, and Online team (ECO, now known as MSEG), I am responsible for working in a team of Solution Managers, UX Researchers, Engineering Leads/PMs, customer and business subject matter experts to determine what is important to the customer/user and understand the experience (workflows, customer/user journey), target low level drivers, measurements, metrics and success drivers. I work with engineering teams to help design instrumentation needs for engineering systems and improve data acquisition and collection strategy by doing gap analysis.

 Deliverables from this project include: increase of support.microsoft.com NSAT by 6\%, developed a mathemtically-derived NSAT backup system for support.microsoft.com, developed a machine learning algorithm deployment infrastructure, text mining analysis of survey data, proof-of-concept chat wait time predictive model, call center performance analysis, analysis of over 200TB of COSMOS data to determine business use case volume, trained in FY15 privacy 101, privacy 201: mps fundamentals.
 
General duties include conducting statistical analysis to determine key factors for planning and conducting experiments to prove causality using prescriptive and predictive analytics by application of appropriate machine learning algorithms (decision tree, classification, clustering, regression tree, logistic regression, random forest, regression etc.) 

 I am also responsible for A/B Testing of experiments designed to validate clear hypotheses regarding measurable outcomes, and for modelling the telemetric requirements for the KPIs, drivers and True North Metrics for any strategic priority. Additional duties include: analytical model management in databases between COSMOS, Hive, SSAS data cubes, and HDInsight clusters running on Azure.

 Responsible for adhoc data queries using SQL scripting, and adhoc data integration in MSIT provided servers by building Data models using SSAS and C\#. I am also conducting statistical analysis to determine key factors for planning and conducting experiments to prove causality using statistical tools such as R, Matlab, JMP and Data Visualization tools such as SSRS, PowerView.


#### Data & Decision Sciences Group 
Led trainings on data science and analytics best practices. Provided cross-product org insightswith R, SAS, Python, and JMP. Built models to detect at-risk product keys and global piracy patterns and built regime-changing models to predict code signing time for large builds.

I was also responsible for using the group's Hadoop cluster to architect, design, and develop actionable analytics with associated Hadoop technologies like Pig and Hive. 

 Delivered a training course to senior management level employees on topics including: Theoretical Probability Distributions, Hypothesis testing, Correlation, Experimentation Principles, and tutorials on specific tools like Sigma-XL, Minitab, JMP, and Excel.

Delivered a training course to senior management level employees on topics in data visualization, such as: design principles for visualization, basic and advanced visualization principles, analysis principles for data visualization, correlation, data exploration, and big data analytics. Tutorials on specific tools like powerview and pivot charts were done in small groups with attendees.

 At Microsoft I worked in the Data and Decision Sciences Group on numerous statistical Big-Data projects. The group interfaced with many internal corporate clients to provide analytical insights with the use of R, SAS, Python, and JMP. I was also responsible for using the group's Hadoop cluster to produce actionable analytics with associated Hadoop technologies like Pig and Hive. I also edited and helped present training material for the group's seminars to upper management on data visualization and statistics for business decision making.

-----





## Education

* MSc Astrophysics, University College London, 2012
* BS Physics, Western Washington University, 2010

## Publications
### [An Introduction to Machine Learning with R - O'Reilly Media (2018)](https://www.amazon.com/Introduction-Machine-Learning-Rigorous-Mathematical/dp/1491976446)
Introductory text book on the basics of machine learning with the R language. Topic covered
include regression, classification, neural networks, tree-based models, and deep dives into common
machine learning packages that are used with R. Used in many univeristy courses on statistics.

## Technical Skills
Alation, Apache Airflow, Apache Superset, AWS Athena, AWS Aurora, AWS Redshift, Amplitude, Git, Google Analytics, Google BigQuery, Jira, LATEX, PowerBI, Python, R, Splunk, SQL,
Tableau, Testrail

## Invited Talks
__[Engineering Analytics at Qualtrics](https://getdx.com/podcast/)__ - (June 2024 planned release) Interview with Abi Noda for the Engineering Enablement Podcast. Topics covered included career journey, challenges with OKR data, cross-department analytics communications, Jira data strategy, and many others.

__[The Data Science of Roguelikes](https://www.youtube.com/watch?v=IliQZm5Itng)__ - Invited talk for the 2023 Roguelike Celebration. Talk covered videogame data sources, API integrations with Steam, and analyses of the genre landscape for roguelike games. Conclusions were that Steam has the most robust dataset, the state of the genre is growing, and that similar game recommendations can be found via game tag cosine similarity scores.

__[Computer Chess & Data Science](https://www.youtube.com/watch?v=mge035g8lf8)__ - Invited talk for the Pickford Film Center in Bellingham, WA 2021. Talk covered early chess history, digital computer game history, the minimax algorithm, alpha-beta pruning, computer chess tournaments, and human-computer chess competitions.  

__[Attribution Modelling 101](https://www.youtube.com/watch?v=E0ObToWagzk)__ - Invited talk for the Dynamic Talks series in Redmond, WA 2019. Topic covered include: customer journey analytics, intro to BigQuery, table sharding and hit-level user data, attribution models in R, model outputs and reccomendations.

__[The Data Science of Board Games](https://svburger.com/2017/09/03/info-for-attendees-of-my-pax-talk/)__ - Penny Arcade Expo, Seattle 2017. Board games have seen an unprecedented boom in their popularity recently. In this talk ,I walked through data on over 250,000 board games from the beginning of civilization to modern times to chart the trends, looking at insights about ratings, balance, and guilty pleasures hidden in the data to shed light on this booming sector of gaming.

__User-Based Reviews and the PC Gaming Ecosytem: What It Takes to Survive in the Videogame World of 2015__ - The Data Science Conference, Chicago 2015. At this conference I detailed a method for quantifying emotions and polarity of text-based, user-submitted reviews through the PC gaming digital distribution service, Steam. In my work, I showed that the current algorithms in use are not well suited for aggregating user emotion, as the algorithm is highly dictionary-dependant for word categorization. However, word polarity aggregation appeared to be a worthwhile metric to investigate.


## Other Projects
__FAA Private Pilot's License - 4826074__ - In December 2023, I passed my Cessna 152 checkride to achieve my VFR private pilot's license.

__[SVBurger.com](https://svburger.com/)__ - Data science and analytics blog maintained since 2017. Notable topics include developing a python library for Steam game data analysis, user interest behavioral statistics, bike race clustering solutions to categorical alignment problems, and potential analytical frameworks that can be better suited for analysis than classical systems like NSAT.
